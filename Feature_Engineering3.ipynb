{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72095228-29f9-474c-a25a-11cb99bf0a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
      "application.\n",
      "\n",
      "Ans.Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform features to a specific range, typically [0, 1]. It adjusts the values in the dataset by scaling the minimum value to 0 and the maximum value to 1. The formula for Min-Max scaling is:\n",
      "\n",
      "X' = x'-x_min/x_max-x_min\n",
      "\n",
      "where X is the original value, x_min is the minimum value in the dataset, and X_max is the maximum value in the dataset.\n",
      "\n",
      "Example:\n",
      "Suppose we have the following dataset:\n",
      "[10, 20, 30, 40, 50]\n",
      "\n",
      "To normalize this data using Min-Max scaling:\n",
      "\n",
      "1. Find the minimum value: ( X_{min} = 10 )\n",
      "2. Find the maximum value: ( X_{max} = 50 )\n",
      "3. Apply the Min-Max scaling formula to each value:\n",
      "Normalized dataset:\n",
      "[0, 0.25, 0.5, 0.75, 1]Min-Max scaling is used to ensure that all features contribute equally to the analysis and to improve the performance of machine learning algorithms, especially those that are sensitive to the scale of data, such as gradient descent-based methods.\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n",
    "\n",
    "Ans.Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform features to a specific range, typically [0, 1]. It adjusts the values in the dataset by scaling the minimum value to 0 and the maximum value to 1. The formula for Min-Max scaling is:\n",
    "\n",
    "X' = x'-x_min/x_max-x_min\n",
    "\n",
    "where X is the original value, x_min is the minimum value in the dataset, and X_max is the maximum value in the dataset.\n",
    "\n",
    "Example:\n",
    "Suppose we have the following dataset:\n",
    "[10, 20, 30, 40, 50]\n",
    "\n",
    "To normalize this data using Min-Max scaling:\n",
    "\n",
    "1. Find the minimum value: ( X_{min} = 10 )\n",
    "2. Find the maximum value: ( X_{max} = 50 )\n",
    "3. Apply the Min-Max scaling formula to each value:\n",
    "Normalized dataset:\n",
    "[0, 0.25, 0.5, 0.75, 1]Min-Max scaling is used to ensure that all features contribute equally to the analysis and to improve the performance of machine learning algorithms, especially those that are sensitive to the scale of data, such as gradient descent-based methods.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f292750c-9b37-447c-8ac8-958fa6af246a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
      "Provide an example to illustrate its application.\n",
      "\n",
      "Ans.The Unit Vector technique, also known as normalization to unit norm, is a feature scaling method that scales each data point (or feature vector) so that it has a unit (Euclidean) length of 1. This is done by dividing each element of the vector by the Euclidean norm (length) of the vector. The formula for normalizing a vector \\( \\mathbf{X} \\) is:\n",
      "\n",
      "math{X'} = frac{math{X}}{mathbf{X}_2}\n",
      "\n",
      "where (math{X}_2 is the Euclidean norm of the vector math{X}.\n",
      "\n",
      "Difference from Min-Max Scaling:\n",
      "- Min-Max Scaling** adjusts values to a specific range, typically [0, 1].\n",
      "- Unit Vector Scaling** adjusts values so that the entire vector has a unit length, without constraining the individual values to a specific range.\n",
      "\n",
      "Example:\n",
      "Suppose we have a vector:\n",
      "\n",
      "math{X} = [3, 4]\n",
      "\n",
      "To normalize this vector using the Unit Vector technique:\n",
      "\n",
      "1. Calculate the Euclidean norm (length) of the vector:\n",
      "\n",
      "math{X}_2 = sqrt{3^2 + 4^2} = sqrt{9 + 16} = sqrt{25} = 5\n",
      "\n",
      "2. Divide each element by the Euclidean norm:\n",
      "\n",
      "math{X'} = frac{[3, 4]}{5} = left[ frac{3}{5}, frac{4}{5} right] = [0.6, 0.8] \n",
      "\n",
      "Now, the normalized vector math{X'} has a unit length of 1.\n",
      "Application:\n",
      "Unit Vector scaling is particularly useful in machine learning algorithms that calculate distances between data points, such as K-means clustering and K-nearest neighbors, ensuring that the direction of the vector (rather than its magnitude) is considered.\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n",
    "\n",
    "Ans.The Unit Vector technique, also known as normalization to unit norm, is a feature scaling method that scales each data point (or feature vector) so that it has a unit (Euclidean) length of 1. This is done by dividing each element of the vector by the Euclidean norm (length) of the vector. The formula for normalizing a vector \\( \\mathbf{X} \\) is:\n",
    "\n",
    "math{X'} = frac{math{X}}{mathbf{X}_2}\n",
    "\n",
    "where (math{X}_2 is the Euclidean norm of the vector math{X}.\n",
    "\n",
    "Difference from Min-Max Scaling:\n",
    "- Min-Max Scaling** adjusts values to a specific range, typically [0, 1].\n",
    "- Unit Vector Scaling** adjusts values so that the entire vector has a unit length, without constraining the individual values to a specific range.\n",
    "\n",
    "Example:\n",
    "Suppose we have a vector:\n",
    "\n",
    "math{X} = [3, 4]\n",
    "\n",
    "To normalize this vector using the Unit Vector technique:\n",
    "\n",
    "1. Calculate the Euclidean norm (length) of the vector:\n",
    "\n",
    "math{X}_2 = sqrt{3^2 + 4^2} = sqrt{9 + 16} = sqrt{25} = 5\n",
    "\n",
    "2. Divide each element by the Euclidean norm:\n",
    "\n",
    "math{X'} = frac{[3, 4]}{5} = left[ frac{3}{5}, frac{4}{5} right] = [0.6, 0.8] \n",
    "\n",
    "Now, the normalized vector math{X'} has a unit length of 1.\n",
    "Application:\n",
    "Unit Vector scaling is particularly useful in machine learning algorithms that calculate distances between data points, such as K-means clustering and K-nearest neighbors, ensuring that the direction of the vector (rather than its magnitude) is considered.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f234db07-aab4-445c-8843-929c646c71f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
      "example to illustrate its application.\n",
      "\n",
      "Ans.Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction. It transforms a large set of variables into a smaller one that still contains most of the information in the original set. PCA achieves this by identifying the principal components, which are orthogonal axes that capture the maximum variance in the data.\n",
      "\n",
      "How PCA is Used in Dimensionality Reduction:\n",
      "1. Standardize the Data: Mean centering and scaling.\n",
      "2. Compute the Covariance Matrix: To understand the relationships between variables.\n",
      "3. Compute Eigenvalues and Eigenvectors: These determine the principal components.\n",
      "4. Sort Eigenvalues and Select Top Components: The components with the largest eigenvalues are selected as principal components.\n",
      "5. Transform the Data: Project the original data onto the new principal component axes.\n",
      "\n",
      "Example in Python:\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n",
    "\n",
    "Ans.Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction. It transforms a large set of variables into a smaller one that still contains most of the information in the original set. PCA achieves this by identifying the principal components, which are orthogonal axes that capture the maximum variance in the data.\n",
    "\n",
    "How PCA is Used in Dimensionality Reduction:\n",
    "1. Standardize the Data: Mean centering and scaling.\n",
    "2. Compute the Covariance Matrix: To understand the relationships between variables.\n",
    "3. Compute Eigenvalues and Eigenvectors: These determine the principal components.\n",
    "4. Sort Eigenvalues and Select Top Components: The components with the largest eigenvalues are selected as principal components.\n",
    "5. Transform the Data: Project the original data onto the new principal component axes.\n",
    "\n",
    "Example in Python:\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60a6fd42-2126-414c-9150-2945265578c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.80087751  0.38137043]\n",
      " [ 2.72444731 -0.07836421]\n",
      " [-0.67367145 -0.06792549]\n",
      " [ 0.07081665  0.11691869]\n",
      " [-1.32071501 -0.35199942]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Example data: 5 samples with 3 features\n",
    "data = np.array([[2.5, 2.4, 3.5], [0.5, 0.7, 1.1], [2.2, 2.9, 3.1], [1.9, 2.2, 2.8], [3.1, 3.0, 3.2]])\n",
    "\n",
    "# Standardize the data\n",
    "data_meaned = data - np.mean(data, axis=0)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)  # Reduce to 2 dimensions\n",
    "principal_components = pca.fit_transform(data_meaned)\n",
    "\n",
    "print(principal_components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd851c25-3d14-4c6d-bff7-7dc53c226886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
      "Extraction? Provide an example to illustrate this concept.\n",
      "\n",
      "Ans.Relationship between PCA and Feature Extraction\n",
      "PCA is a feature extraction technique that identifies the most significant features (principal components) in the data, which capture the maximum variance. Unlike feature selection, which selects a subset of the original features, PCA creates new features that are linear combinations of the original ones.\n",
      "\n",
      "How PCA is Used for Feature Extraction\n",
      "1. Standardize the Data: Center and scale the data.\n",
      "2. Compute the Principal Components: Determine the directions (principal components) that capture the maximum variance.\n",
      "3. Transform the Data: Project the original data onto the principal components to create a new set of features.\n",
      "Explanation\n",
      "In this example, PCA reduces the original 3 features to 2 principal components. These new features (principal components) are linear combinations of the original features and capture the most significant patterns in the data. Thus, PCA effectively extracts the most informative features, enabling easier analysis and visualization.\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "Ans.Relationship between PCA and Feature Extraction\n",
    "PCA is a feature extraction technique that identifies the most significant features (principal components) in the data, which capture the maximum variance. Unlike feature selection, which selects a subset of the original features, PCA creates new features that are linear combinations of the original ones.\n",
    "\n",
    "How PCA is Used for Feature Extraction\n",
    "1. Standardize the Data: Center and scale the data.\n",
    "2. Compute the Principal Components: Determine the directions (principal components) that capture the maximum variance.\n",
    "3. Transform the Data: Project the original data onto the principal components to create a new set of features.\n",
    "Explanation\n",
    "In this example, PCA reduces the original 3 features to 2 principal components. These new features (principal components) are linear combinations of the original features and capture the most significant patterns in the data. Thus, PCA effectively extracts the most informative features, enabling easier analysis and visualization.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9bc2b82-d07f-43f1-9f61-12d17b93b07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA(n_components=2)\n",
      "[[-0.80087751  0.38137043]\n",
      " [ 2.72444731 -0.07836421]\n",
      " [-0.67367145 -0.06792549]\n",
      " [ 0.07081665  0.11691869]\n",
      " [-1.32071501 -0.35199942]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Example data: 5 samples with 3 features\n",
    "data = np.array([[2.5, 2.4, 3.5], [0.5, 0.7, 1.1], [2.2, 2.9, 3.1], [1.9, 2.2, 2.8], [3.1, 3.0, 3.2]])\n",
    "\n",
    "# Standardize the data\n",
    "data_meaned = data - np.mean(data, axis=0)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)  # Extract 2 principal components\n",
    "\n",
    "principal_components = pca.fit_transform(data_meaned)\n",
    "\n",
    "print(principal_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70a352fe-9c31-419c-a551-cca714d79832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
      "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
      "preprocess the data.\n",
      "\n",
      "Ans.\n",
      "   price    rating  delivery_time\n",
      "0   0.00  0.500000       0.375000\n",
      "1   0.25  0.714286       1.000000\n",
      "2   0.60  0.000000       0.000000\n",
      "3   0.85  0.357143       0.583333\n",
      "4   1.00  1.000000       0.791667\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n",
    "\n",
    "Ans.\"\"\")\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "# Example dataset\n",
    "data = {'price': [10, 15, 22, 27, 30],\n",
    "        'rating': [4.2, 4.5, 3.5, 4.0, 4.9],\n",
    "        'delivery_time': [30, 45, 21, 35, 40]}\n",
    "df = pd.DataFrame(data)\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "print(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98cf02f3-ea9d-4fa5-88cf-69ce3de7bb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
      "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
      "dimensionality of the dataset.\n",
      "\n",
      "Ans.\n",
      "        PC1           PC2\n",
      "0 -3.464102  2.979041e-16\n",
      "1 -1.732051 -9.930137e-17\n",
      "2 -0.000000  0.000000e+00\n",
      "3  1.732051  9.930137e-17\n",
      "4  3.464102  1.986027e-16\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n",
    "\n",
    "Ans.\"\"\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Example dataset with 5 samples and 6 features\n",
    "data = {\n",
    "    'revenue': [500, 600, 700, 800, 900],\n",
    "    'profit': [50, 60, 70, 80, 90],\n",
    "    'assets': [1000, 1100, 1200, 1300, 1400],\n",
    "    'liabilities': [400, 500, 600, 700, 800],\n",
    "    'market_cap': [10000, 11000, 12000, 13000, 14000],\n",
    "    'trading_volume': [2000, 2200, 2400, 2600, 2800]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "pca = PCA(n_components=2)  # Reduce to 2 dimensions\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "print(pca_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "616a498e-497c-4de3-be75-e0894aa956fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
      "values to a range of -1 to 1.\n",
      "\n",
      "Ans.\n",
      "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.\n",
    "\n",
    "Ans.\"\"\")\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "scaled_data = 2 * (data - np.min(data)) / (np.max(data) - np.min(data)) - 1\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f676da6-5b8c-4588-b5d4-a326536e30f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
      "Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
      "\n",
      "Ans.Choosing Principal Components:\n",
      "Retain components that explain at least 95% of the variance.\n",
      "In this example, 4 components explain 95% of the variance.\n",
      "\n",
      "Explanation:\n",
      "Retaining 4 principal components ensures that most of the information (95% variance) is preserved, reducing dimensionality while keeping essential features.\n",
      "\n",
      "\n",
      "Cumulative variance: [0.73432573 0.99205813 0.99888426 1.         1.        ]\n",
      "Number of components to retain for 95% variance: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "Ans.Choosing Principal Components:\n",
    "Retain components that explain at least 95% of the variance.\n",
    "In this example, 4 components explain 95% of the variance.\n",
    "\n",
    "Explanation:\n",
    "Retaining 4 principal components ensures that most of the information (95% variance) is preserved, reducing dimensionality while keeping essential features.\n",
    "\n",
    "\"\"\")\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Example data\n",
    "data = {'height': [170, 160, 180, 175, 165],\n",
    "        'weight': [65, 70, 75, 80, 60],\n",
    "        'age': [25, 30, 35, 40, 20],\n",
    "        'gender': [1, 0, 1, 0, 1],  # 1 for male, 0 for female\n",
    "        'blood_pressure': [120, 125, 130, 135, 110]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Determine explained variance\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = explained_variance.cumsum()\n",
    "print(\"Cumulative variance:\", cumulative_variance)\n",
    "\n",
    "# Determine number of components to retain (e.g., for 95% variance)\n",
    "n_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "print(f\"Number of components to retain for 95% variance: {n_components}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3430dae1-b584-4ab7-960a-4a3cf06223aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
